{
  # Spark config
  spark: {
    app: {
      name: Nebula Exchange
    }
  }

  # if the hive is hive-on-spark with derby modeï¼Œ you can ignore this hive configure
  # get the com.vesoft.exchange.common.config values from file $HIVE_HOME/conf/hive-site.xml or hive-default.xml

  #  hive: {
  #    warehouse: "hdfs://NAMENODE_IP:9000/apps/svr/hive-xxx/warehouse/"
  #    connectionURL: "jdbc:mysql://your_ip:3306/hive_spark?characterEncoding=UTF-8"
  #    connectionDriverName: "com.mysql.jdbc.Driver"
  #    connectionUserName: "user"
  #    connectionPassword: "password"
  #  }


  # Nebula Graph config
  nebula: {
    address:{
      graph:["127.0.0.1:9669"]
      meta:["127.0.0.1:9559"]
    }
    user: root
    pswd: nebula
    space: test

    # if com.vesoft.exchange.common.config graph ssl encrypted transmission
    ssl:{
        # if enable is false, other params of ssl are invalid.
        enable:{
            graph:false
            meta:false
        }
        # ssl sign type: CA or SELF
        sign.type:ca

        # if sign.type is CA, make sure com.vesoft.exchange.common.config the ca.param. If you submit exchange application with cluster, please make sure each worker has the ca files.
        ca.param: {
            caCrtFilePath:"/path/caCrtFilePath"
            crtFilePath:"/path/crtFilePath"
            keyFilePath:"/path/keyFilePath"
        }

        # if sign.type is SELF, make sure com.vesoft.exchange.common.config the self.param. If you submit exchange application with cluster, please make sure each worker has the ca files.
        self.param: {
            crtFilePath:"/path/crtFilePath"
            keyFilePath:"/path/keyFilePath"
            password:"nebula"
        }
    }


    # parameters for SST import, not required
    path:{
        local:"/tmp"
        remote:"/sst"
        hdfs.namenode: "hdfs://name_node:9000"
    }

    # nebula client connection parameters
    connection {
      # socket connect & execute timeout, unit: millisecond
      timeout: 30000
    }

    error: {
      # max number of failures, if the number of failures is bigger than max, then exit the application.
      max: 32
      # failed import job will be recorded in output path
      output: /tmp/errors
    }

    # use google's RateLimiter to limit the requests send to NebulaGraph
    rate: {
      # the stable throughput of RateLimiter
      limit: 1024
      # Acquires a permit from RateLimiter, unit: MILLISECONDS
      # if it can't be obtained within the specified timeout, then give up the request.
      timeout: 1000
    }
  }

  # Processing tags
  # There are tag com.vesoft.exchange.common.config examples for different dataSources.
  tags: [

    # HDFS parquet
    # Import mode is client, just change type.sink to sst if you want to use sst import mode.
    {
      name: tag-name-0
      type: {
        source: parquet
        sink: client
      }
      path: hdfs tag path 0

      fields: [parquet-field-0, parquet-field-1, parquet-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      vertex: {
        field:new-parquet-field
        udf:{
                          separator:"_"
                          oldColNames:[parquet-field-0]
                          newColNames:[new-parquet-field]
                      }
        #policy:hash
      }
      batch: 2000
      partition: 60
    }

    # HDFS csv
    # Import mode is sst, just change type.sink to client if you want to use client import mode.
    {
      name: tag-name-1
      type: {
        source: csv
        sink: sst
      }
      path: hdfs tag path 2
      # if your csv file has no header, then use _c0,_c1,_c2,.. to indicate fields
      fields: [csv-field-0, csv-field-1, csv-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      vertex: {
        field:csv-field-0
      }
      separator: ","
      header: true
      batch: 2000
      partition: 60
      # optional config, default is false
      # config repartitionWithNebula as true means: repartition spark dataframe with nebula partition number to write sst files.
      repartitionWithNebula: false
    }

    # HDFS json
    {
      name: tag-name-2
      type: {
        source: json
        sink: client
      }
      path: hdfs vertex path 3
      fields: [json-field-0, json-field-1, json-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      vertex: {
        field: json-field-0
        #policy: hash
      }
      batch: 2000
      partition: 60
    }

    # Hive
    {
      name: tag-name-3
      type: {
        source: hive
        sink: client
      }
      exec: "select hive-field0, hive-field1, hive-field2 from database.table"
      fields: [hive-field-0, hive-field-1, hive-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      vertex: {
        field: hive-field-0
        # policy: "hash"
      }
      batch: 2000
      partition: 60
    }

    # neo4j
    {
      name: tag-name-4
      type: {
        source: neo4j
        sink: client
      }
      server: "bolt://127.0.0.1:7687"
      user: neo4j
      password: neo4j
      exec: "match (n:label) return n.neo4j-field-0 as neo4j-field-0, n.neo4j-field-1 as neo4j-field-1 order by (n.neo4j-field-0)"
      fields: [neo4j-field-0, neo4j-field-1]
      nebula.fields: [nebula-field-0, nebula-field-1]
      vertex: {
        field:neo4j-field-0
        # policy:hash
      }
      partition: 60
      batch: 2000
      check_point_path: /tmp/test
   }

    # HBase
    # if fields or vertex contains rowkey, please configure it as "rowkey".
    {
      name: tag-name-5
      type: {
        source: hbase
        sink: client
      }
      host:127.0.0.1
      port:2181
      table:hbase-table
      columnFamily:hbase-table-cloumnfamily
      fields: [hbase-column-0, hbase-column-1]
      nebula.fields: [nebula-field-0, nebula-field-1]
      vertex: {
        field:rowkey
      }
      partition: 60
      batch: 2000
    }

    # Pulsar
    {
      name: tag-name-6
      type: {
        source: pulsar
        sink: client
      }
      service: "pulsar://localhost:6650"
      admin: "http://localhost:8081"
      options: {
        # choose one of "topic", "topics", "topicsPattern"
        topics: "topic1,topic2"
      }
      fields: [pulsar-field-0, pulsar-field-1, pulsar-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      vertex: {
        field:pulsar-field-0
      }
      partition: 60
      batch: 2000
      interval.seconds: 10
    }

    # KAFKA
    {
      name: tag-name-7
      type: {
        source: kafka
        sink: client
      }
      service: "kafka.service.address"
      topic: "topic-name"
      fields: [kafka-field-0, kafka-field-1, kafka-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      vertex: {
        field: kafka-field-0
      }
      partition: 60
      batch: 2000
      interval.seconds: 10
    }

    # MaxCompute
    {
      name: tag-name-8
      type:{
        source:maxcompute
        sink:client
      }
      table:table
      project:project
      odpsUrl:"http://service.cn-hangzhou.maxcompute.aliyun.com/api"
      tunnelUrl:"http://dt.cn-hangzhou.maxcompute.aliyun.com"
      accessKeyId:xxx
      accessKeySecret:xxx
      partitionSpec:"dt='partition1'"
      # default numPartitions is 1
      numPartitions:100
      # maxcompute sql sentence only uses table name. make sure that table name is the same with {table}'s value'.
      sentence:"select id, maxcompute-field-0, maxcompute-field-1, maxcompute-field-2 from table where id < 10"
      fields:[maxcompute-field-0, maxcompute-field-1]
      nebula.fields:[nebula-field-0, nebula-field-1]
      vertex:{
        field: maxcompute-field-2
      }
      partition:60
      batch:2000
    }

    # ClickHouse
   {
      name: tag-name-8
      type: {
        source: clickhouse
        sink: client
      }
      url:"jdbc:clickhouse://127.0.0.1:8123/database"
      user:"user"
      password:"clickhouse"
      numPartition:"5"
      table:"table"
      sentence:"select * from table"
      fields: [clickhouse-field-0, clickhouse-field-1, clickhouse-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      vertex: {
        field:clickhouse-field-0
        #policy:hash
      }
      batch: 2000
      partition: 60
    }

    # PostgreSQL
    {
      name: tag9
      type: {
        source: postgresql
        sink: client
      }
      user:root
      host: "127.0.0.1"
      port: "5432"
      database: "database"
      table: "table"
      user: "root"
      password: "nebula"
      sentence: "select postgre-field0, postgre-field1, postgre-field2 from table"
      fields: [postgre-field-0, postgre-field-1, postgre-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      vertex: {
        field: postgre-field-0
        # policy: "hash"
      }
      batch: 2000
      partition: 60
    }

    # Oracle
    {
      name: tag10
      type: {
        source: oracle
        sink: client
      }
      url:"jdbc:oracle:thin:@host:1521:db"
      driver: "oracle.jdbc.driver.OracleDriver"
      user: "root"
      password: "nebula"
      table: "db.table"
      sentence: "select oracle-field0, oracle-field1, oracle-field2 from table"
      fields: [oracle-field-0, oracle-field-1, oracle-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      vertex: {
        field: oracle-field-0
        # policy: "hash"
      }
      batch: 2000
      partition: 60
    }
  ]

  # Processing edges
  # There are edge com.vesoft.exchange.common.config examples for different dataSources.
  edges: [
     # HDFS parquet
     # Import mode is client, just change type.sink to sst if you want to use sst import mode.
    {
      name: edge-name-0
      type: {
         source: parquet
         sink: client
      }
      path: hdfs edge path 0
      fields: [parquet-field-0, parquet-field-1, parquet-field-2]
      nebula.fields: [nebula-field-0 nebula-field-1 nebula-field-2]
      source: {
        field:parquet-field-0
        udf:{
                          separator:"_"
                          oldColNames:[parquet-field-0]
                          newColNames:[new-parquet-field]
                      }
        #policy:hash
      }
      target: {
        field:parquet-field-1
        udf:{
                          separator:"_"
                          oldColNames:[parquet-field-0]
                          newColNames:[new-parquet-field]
                      }
        #policy:hash
      }
      ranking: parquet-field-2
      batch: 2000
      partition: 60
    }

    # HDFS csv
    {
      name: edge-name-1
      type: {
        source: csv
        sink: client
      }
      path: hdfs edge path 1
      fields: [csv-field-0, csv-field-1, csv-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      source: {
        field: csv-field-0
        #policy: hash
      }
      target: {
        field: csv-field-1
      }
      ranking: csv-field-2
      separator: ","
      header: true
      batch: 2000
      partition: 60
      # optional config, default is false
      # config repartitionWithNebula as true means: repartition spark dataframe with nebula partition number to write sst files.
      repartitionWithNebula: false
    }

    # HDFS json
    {
      name: edge-name-2
      type: {
        source: json
        sink: client
      }
      path: hdfs edge path 2
      fields: [json-field-0, json-field-1, json-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      source: {
        field: json-field-0
        #policy: hash
      }
      target: {
        field: json-field-1
      }
      ranking: json-field-2
      batch: 2000
      partition: 60
    }

    # Hive
    {
      name: edge-name-2
      type: {
        source: hive
        sink: client
      }
      exec: "select hive-field0, hive-field1, hive-field2 from database.table"
      fields: [ hive-field-0, hive-field-1, hive-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      source: hive-field-0
      target: hive-field-1
      ranking: hive-filed-2
      batch: 2000
      partition: 60
    }

    # Neo4j
    {
      name: edge-name-3
      type: {
        source: neo4j
        sink: client
      }
      server: "bolt://127.0.0.1:7687"
      user: neo4j
      password: neo4j
      exec: "match (a:vertex_label)-[r:edge_label]->(b:vertex_label) return a.neo4j-source-field, b.neo4j-target-field, r.neo4j-field-0 as neo4j-field-0, r.neo4j-field-1 as neo4j-field-1 order by id(r)"
      fields: [neo4j-field-0, neo4j-field-1]
      nebula.fields: [nebula-field-0, nebula-field-1]
      source: {
        field: a.neo4j-source-field
      }
      target: {
        field: b.neo4j-target-field
      }
      ranking: neo4j-field-2
      partition: 60
      batch: 2000
      check_point_path: /tmp/test
    }

    # HBase
    {
      name: edge-name-4
      type: {
        source: hbase
        sink: client
      }
      host:127.0.0.1
      port:2181
      table:hbase-table
      columnFamily:hbase-table-cloumnfamily
      fields: [hbase-column-0, hbase-column-1]
      nebula.fields:[nebula-field-0, nebula-field-1]
      source: {
        field: hbase-column-k
      }
      target: {
        field: hbase-column-h
      }
      ranking: hbase-column-t
      partition: 60
      batch: 2000
    }


    # Pulsar
    {
      name: edge-name-5
      type: {
        source: pulsar
        sink: client
      }
      service: "pulsar://localhost:6650"
      admin: "http://localhost:8081"
      options: {
        # choose one of "topic", "topics", "topicsPattern"
        topic: "topic1"
      }
      fields: [pulsar-field-0, pulsar-field-1, pulsar-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      source: {
        field: pulsar-field-0
        #policy: hash
      }
      target: {
        field: pulsar-field-1
      }
      ranking: pulsar-field-2
      partition: 60
      batch: 2000
      interval.seconds: 10
    }

    # KAFKA
    {
      name: edge-name-6
      type: {
        source: kafka
        sink: client
      }
      service: "kafka.service.address"
      topic: "topic-name"
      fields: [kafka-field-0, kafka-field-1, kafka-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      source: kafka-field-0
      target: kafka-field-1
      ranking: kafka-field-2
      partition: 60
      batch: 2000
      interval.seconds: 10
    }

    # MaxCompute
    {
      name: edge-name-7
      type:{
        source:maxcompute
        sink:client
      }
      table:table
      project:project
      odpsUrl:"http://service.cn-hangzhou.maxcompute.aliyun.com/api"
      tunnelUrl:"http://dt.cn-hangzhou.maxcompute.aliyun.com"
      accessKeyId:xxx
      accessKeySecret:xxx
      partitionSpec:"dt='partition1'"
      # maxcompute sql sentence only uses table name.
      sentence:"select * from table"
      fields:[maxcompute-field-0, maxcompute-field-1]
      nebula.fields:[nebula-field-0, nebula-field-1]
      source:{
        field: maxcompute-field-2
      }
      target:{
        field: maxcompute-field-3
      }
      ranking: maxcompute-field-4
      partition:60
      batch:2000
    }

    # ClickHouse
   {
      name: edge-name-7
      type: {
        source: clickhouse
        sink: client
      }
      url:"jdbc:clickhouse://127.0.0.1:8123/database"
      user:"user"
      password:"clickhouse"
      numPartition:"5"
      table:"table"
      sentence:"select * from table"
      fields: [clickhouse-field-2]
      nebula.fields: [nebula-field-2]
      source: {
        field:clickhouse-field-0
        #policy:hash
      }
      target: {
        field:clickhouse-field-1
        #policy:hash
      }
      ranking:clickhouse-field-3
      batch: 2000
      partition: 60
    }

    # PostgreSQL
    {
      name: edge-name-8
      type: {
        source: postgresql
        sink: client
      }
      user:root
      host: "127.0.0.1"
      port: "5432"
      database: "database"
      table: "table"
      user: "root"
      password: "nebula"
      sentence: "select postgre-field0, postgre-field1, postgre-field2 from table"
      fields: [postgre-field-0, postgre-field-1, postgre-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      source: {
        field: mysql-field-0
        # policy: "hash"
      }
      target: {
        field: mysql-field-0
        # policy: "hash"
      }
      ranking: postgre-field-1
      batch: 2000
      partition: 60
    }

    # Oracle
    {
      name: edge-name-9
      type: {
        source: oracle
        sink: client
      }
      url:"jdbc:oracle:thin:@host:1521:db"
      driver: "oracle.jdbc.driver.OracleDriver"
      user: "root"
      password: "nebula"
      table: "db.table"
      sentence: "select oracle-field0, oracle-field1, oracle-field2 from table"
      fields: [oracle-field-0, oracle-field-1, oracle-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      source: {
        field: oracle-field-0
        # policy: "hash"
      }
      target: {
        field: oracle-field-1
      }
      ranking: oracle-field-2
      batch: 2000
      partition: 60
    }
  ]
}
